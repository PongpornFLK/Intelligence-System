import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pickle
import os , re
import matplotlib.pyplot as plt
from TrainModel.mlp import load_neural_model
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error

# üìå ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Paths ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ
MODEL_DIR = "TrainModel"
MODEL_PATH = os.path.join(MODEL_DIR, "load_model.h5")
SCALER_PATH = os.path.join(MODEL_DIR, "scaler.pkl")
COLUMNS_PATH = os.path.join(MODEL_DIR, "train_columns.pkl")
HISTORY_PATH = os.path.join(MODEL_DIR, "load_history.pkl")

st.title("üìä Neural Network Model")
st.subheader("‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•")

# üöÄ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ Train ‡πÉ‡∏´‡∏°‡πà
if not os.path.exists(MODEL_PATH) or not os.path.exists(HISTORY_PATH):
    st.warning("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á Train ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•... ‡πÇ‡∏õ‡∏£‡∏î‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà üöÄ")
    load_neural_model()  # Train ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    st.success("‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏• Train ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!")

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
model = keras.models.load_model(MODEL_PATH, custom_objects={"mse": keras.losses.MeanSquaredError()})
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
              loss='mse',
              metrics=['mae'])

# ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ Train
with open(HISTORY_PATH, "rb") as f:
    history = pickle.load(f)

# üöÄ 1Ô∏è‚É£ ‡πÅ‡∏™‡∏î‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
st.markdown("### üîß ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•")
stringlist = []  # ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
model.summary(print_fn=lambda x: stringlist.append(x))

# ‡πÅ‡∏™‡∏î‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
st.markdown("```\n" + "\n".join(stringlist) + "\n```")

# ‡πÉ‡∏ä‡πâ Regular Expression ‡∏à‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å model.summary
pattern = r"(\S+)\s+(\S+)\s+(\([\d, None]+\))\s+([\d,]+)"
summary_data = []

for line in stringlist[1:-4]:  # ‡∏Ç‡πâ‡∏≤‡∏°‡∏´‡∏±‡∏ß‡πÅ‡∏•‡∏∞‡∏ó‡πâ‡∏≤‡∏¢
    match = re.match(pattern, line)
    if match:
        summary_data.append(match.groups())

# üöÄ 2Ô∏è‚É£ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö
st.markdown("### üéØ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å Test Set")
try:
    with st.spinner("üîÑ Loading and Training Model"):
        df_test = pd.read_csv(r'Data_set/education_career_bad_model.csv')

        # ‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Cleaning)
        df_test_cleaned = df_test.drop(columns=["Student_ID", "Unnamed: 11"], errors="ignore")
        for col in df_test_cleaned.select_dtypes(include=["float64", "int64"]).columns:
            df_test_cleaned[col] = df_test_cleaned[col].fillna(df_test_cleaned[col].mean())
        for col in df_test_cleaned.select_dtypes(include=["object"]).columns:
            df_test_cleaned[col] = df_test_cleaned[col].fillna(df_test_cleaned[col].mode()[0])

        # ‚úÖ ‡πÇ‡∏´‡∏•‡∏î Training Columns
        if os.path.exists(COLUMNS_PATH):
            with open(COLUMNS_PATH, "rb") as f:
                train_columns = pickle.load(f)

            for col in train_columns:
                if col not in df_test_cleaned.columns:
                    df_test_cleaned[col] = 0

            df_test_cleaned = df_test_cleaned[train_columns]
        else:
            st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå train_columns.pkl ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡πà‡∏≠‡∏ô")
            st.stop()

        # ‚úÖ ‡πÇ‡∏´‡∏•‡∏î Scaler
        if os.path.exists(SCALER_PATH):
            with open(SCALER_PATH, "rb") as f:
                scaler = pickle.load(f)
            st.success("‚úÖ ‡πÇ‡∏´‡∏•‡∏î Scaler ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        else:
            st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå scaler.pkl ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡πà‡∏≠‡∏ô")
            st.stop()

        # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        X_test = df_test_cleaned
        X_test_scaled = scaler.transform(X_test)
        y_test = np.log1p(df_test["Starting_Salary"])

        # ‚úÖ ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•
        y_pred = model.predict(X_test_scaled)

        # ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
        df_results = pd.DataFrame({
            "‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á (Actual Salary)": y_test.values[:10],
            "‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ (Predicted Salary)": np.round(y_pred[:10].flatten(), 2)
        })
        st.dataframe(df_results)

        # üöÄ 3Ô∏è‚É£ ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü Loss & MAE ‡∏à‡∏≤‡∏Å Training
        st.markdown("### üìà ‡∏Å‡∏£‡∏≤‡∏ü Training Loss & MAE")
        fig, ax = plt.subplots(1, 2, figsize=(12, 4))

        # ‚úÖ ‡∏Å‡∏£‡∏≤‡∏ü Loss
        ax[0].plot(history["loss"], label="Train Loss")
        ax[0].plot(history["val_loss"], label="Validation Loss")
        ax[0].set_title("Training & Validation Loss")
        ax[0].set_xlabel("Epochs")
        ax[0].set_ylabel("Loss")
        ax[0].legend()

        # ‚úÖ ‡∏Å‡∏£‡∏≤‡∏ü MAE
        ax[1].plot(history["mae"], label="Train MAE")
        ax[1].plot(history["val_mae"], label="Validation MAE")
        ax[1].set_title("Training & Validation MAE")
        ax[1].set_xlabel("Epochs")
        ax[1].set_ylabel("Mean Absolute Error")
        ax[1].legend()

        st.pyplot(fig)
        st.success("‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏• Train ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢! üöÄ")

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN ‡πÉ‡∏ô y_test ‡πÅ‡∏•‡∏∞ y_pred
        y_test_cleaned = y_test[~np.isnan(y_test)]  # ‡∏•‡∏ö‡∏Ñ‡πà‡∏≤ NaN ‡πÉ‡∏ô y_test
        y_pred_cleaned = y_pred[~np.isnan(y_test)]  # ‡∏•‡∏ö‡∏Ñ‡πà‡∏≤ NaN ‡πÉ‡∏ô y_pred


        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ MSE, MAE, R¬≤, RMSE, MAPE ‡πÅ‡∏•‡∏∞‡∏≠‡∏∑‡πà‡∏ô ‡πÜ
        test_loss = model.evaluate(X_test_scaled, y_test_cleaned, verbose=0)[0]
        mse = mean_squared_error(y_test_cleaned, y_pred_cleaned)
        mae = mean_absolute_error(y_test_cleaned, y_pred_cleaned)
        r2 = r2_score(y_test_cleaned, y_pred_cleaned)
        rmse = np.sqrt(mse)
        mape = mean_absolute_percentage_error(y_test_cleaned, y_pred_cleaned)

        col1 , col2 = st.columns(2)
        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
        st.markdown("### üìä Evaluating Model Performance")

        with col1:
            st.write(f"üîπ **Test Loss (MSE):** {test_loss:.4f}")
            st.write(f"üîπ **Mean Absolute Error (MAE):** {mae:.4f}")
            st.write(f"üîπ **Mean Squared Error (MSE):** {mse:.4f}")
        with col2:
            st.write(f"üîπ **Root Mean Squared Error (RMSE):** {rmse:.4f}")
            st.write(f"üîπ **Mean Absolute % Error (MAPE):** {mape:.4f}")
            st.write(f"üîπ **R¬≤ Score (R2):** {r2:.4f}")



        df_metrics = pd.DataFrame({
            "Metric": ["Test Loss (MSE)", "Mean Absolute Error (MAE)", "Mean Squared Error (MSE)",
                           "Root Mean Squared Error (RMSE)", "Mean Absolute Percentage Error (MAPE)", "R¬≤ Score (R2)"],
            "Value": [test_loss, mae, mse, rmse, mape, r2]
        })
        st.dataframe(df_metrics)
except Exception as e:
    st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")


