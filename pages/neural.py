import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pickle
import os
import matplotlib.pyplot as plt
from TrainModel.mlp import load_neural_model
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error

st.title("ğŸ“Š Neural Network Model (Load)")
st.subheader("ğŸ” à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸ˆà¸²à¸à¸à¸²à¸£ Train à¹‚à¸¡à¹€à¸”à¸¥")

# ğŸš€ à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µà¹‚à¸¡à¹€à¸”à¸¥à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¹ƒà¸«à¹‰ Train à¹ƒà¸«à¸¡à¹ˆ
if not os.path.exists("load_model.h5") or not os.path.exists("load_history.pkl"):
    st.warning("â³ à¸à¸³à¸¥à¸±à¸‡ Train à¸‚à¹‰à¸­à¸¡à¸¹à¸¥... à¹‚à¸›à¸£à¸”à¸£à¸­à¸ªà¸±à¸à¸„à¸£à¸¹à¹ˆ ğŸš€")
    load_neural_model()  # Train à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
    st.success("âœ… à¹‚à¸¡à¹€à¸”à¸¥ Train à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§!")

# âœ… à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸µà¹ˆà¸à¸¶à¸à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§
model = keras.models.load_model("load_model.h5", custom_objects={"mse": keras.losses.MeanSquaredError()})

# âœ… à¹‚à¸«à¸¥à¸”à¸›à¸£à¸°à¸§à¸±à¸•à¸´à¸à¸²à¸£ Train
with open("load_history.pkl", "rb") as f:
    history = pickle.load(f)

# ğŸš€ 1ï¸âƒ£ à¹à¸ªà¸”à¸‡à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¸­à¸‡à¹‚à¸¡à¹€à¸”à¸¥à¸”à¹‰à¸§à¸¢ model.summary()
st.markdown("### ğŸ”§ à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¸­à¸‡à¹‚à¸¡à¹€à¸”à¸¥")
stringlist = []
model.summary(print_fn=lambda x: stringlist.append(x))
model_summary = "\n".join(stringlist)
st.text(model_summary)

# Convert model summary to table format
summary_lines = model_summary.split('\n')
summary_lines = [line for line in summary_lines if line.strip() != '']
summary_data = []
for line in summary_lines[1:-4]:
    summary_data.append(line.split())

# Ensure all rows have the same number of columns
max_cols = max(len(row) for row in summary_data)
for row in summary_data:
    while len(row) < max_cols:
        row.append('')

summary_table = pd.DataFrame(summary_data, columns=summary_lines[0].split())
st.table(summary_table)

# ğŸš€ 2ï¸âƒ£ à¸—à¸³à¸™à¸²à¸¢à¸œà¸¥à¸ˆà¸²à¸à¸Šà¸¸à¸” Test Set
st.markdown("### ğŸ¯ à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¸ˆà¸²à¸ Test Set")
df_test = pd.read_csv(r'Data_set/education_career_bad_model.csv')

# âœ… à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (Data Cleaning) à¹€à¸Šà¹ˆà¸™à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸šà¸•à¸­à¸™ Train
df_test_cleaned = df_test.drop(columns=["Student_ID", "Unnamed: 11"], errors="ignore")
for col in df_test_cleaned.select_dtypes(include=["float64", "int64"]).columns:
    df_test_cleaned[col].fillna(df_test_cleaned[col].mean(), inplace=True)
for col in df_test_cleaned.select_dtypes(include=["object"]).columns:
    df_test_cleaned[col].fillna(df_test_cleaned[col].mode()[0], inplace=True)

# à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Categorical à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ One-Hot Encoding
categorical_cols_test = df_test_cleaned.select_dtypes(include=['object']).columns.tolist()
df_test_cleaned = pd.get_dummies(df_test_cleaned, columns=categorical_cols_test, drop_first=True)

# à¹‚à¸«à¸¥à¸” scaler à¸—à¸µà¹ˆà¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸§à¹‰
scaler_path = "TrainModel/scaler.pkl"
if os.path.exists(scaler_path):
    with open(scaler_path, "rb") as f:
        scaler = pickle.load(f)
else:
    scaler = None
    st.error("à¹„à¸¡à¹ˆà¸à¸š scaler.pkl")

# à¹€à¸•à¸£à¸µà¸¢à¸¡à¸Šà¸¸à¸” Features à¸ªà¸³à¸«à¸£à¸±à¸š Test Set
X_test = df_test_cleaned.drop(columns=["Starting_Salary"], errors="ignore")
if scaler is not None:
    X_test_scaled = scaler.transform(X_test)
    y_test = np.log1p(df_test_cleaned["Starting_Salary"])

    # à¸—à¸³à¸™à¸²à¸¢à¸œà¸¥
    y_pred = model.predict(X_test_scaled)

    # à¹à¸ªà¸”à¸‡à¸„à¹ˆà¸²à¸ˆà¸£à¸´à¸‡à¹à¸¥à¸°à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢ (à¹€à¸‰à¸à¸²à¸° 10 à¹à¸–à¸§à¹à¸£à¸)
    df_results = pd.DataFrame({
        "à¸„à¹ˆà¸²à¸ˆà¸£à¸´à¸‡ (Actual Salary)": y_test.values[:10],
        "à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸—à¸³à¸™à¸²à¸¢ (Predicted Salary)": np.round(y_pred[:10].flatten(), 2)
    })
    st.dataframe(df_results)

    # # ğŸš€ 3ï¸âƒ£ à¸§à¸²à¸”à¸à¸£à¸²à¸Ÿ Loss & MAE à¸ˆà¸²à¸ Training
    # st.markdown("### ğŸ“ˆ à¸à¸£à¸²à¸Ÿ Training Loss & MAE")
    # fig, ax = plt.subplots(1, 2, figsize=(12, 4))

    # # à¸à¸£à¸²à¸Ÿ Loss
    # ax[0].plot(history["loss"], label="Train Loss")
    # ax[0].plot(history["val_loss"], label="Validation Loss")
    # ax[0].set_title("Training & Validation Loss")
    # ax[0].set_xlabel("Epochs")
    # ax[0].set_ylabel("Loss")
    # ax[0].legend()

    # # à¸à¸£à¸²à¸Ÿ MAE
    # ax[1].plot(history["mae"], label="Train MAE")
    # ax[1].plot(history["val_mae"], label="Validation MAE")
    # ax[1].set_title("Training & Validation MAE")
    # ax[1].set_xlabel("Epochs")
    # ax[1].set_ylabel("Mean Absolute Error")
    # ax[1].legend()

    # st.pyplot(fig)

    st.success("âœ… à¹‚à¸¡à¹€à¸”à¸¥ Train à¹€à¸ªà¸£à¹‡à¸ˆà¹€à¸£à¸µà¸¢à¸šà¸£à¹‰à¸­à¸¢! ğŸš€")

    # ğŸš€ à¸„à¸³à¸™à¸§à¸“à¹à¸¥à¸°à¹à¸ªà¸”à¸‡à¸œà¸¥à¸à¸²à¸£à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¹‚à¸¡à¹€à¸”à¸¥
    test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mse)
    mape = mean_absolute_percentage_error(y_test, y_pred)

    st.markdown("### ğŸ“Š Evaluating Model Performance")
    st.write(f"ğŸ”¹ **Test Loss (MSE):** {test_loss:.4f}")
    st.write(f"ğŸ”¹ **Mean Absolute Error (MAE):** {mae:.4f}")
    st.write(f"ğŸ”¹ **Mean Squared Error (MSE):** {mse:.4f}")
    st.write(f"ğŸ”¹ **Root Mean Squared Error (RMSE):** {rmse:.4f}")
    st.write(f"ğŸ”¹ **Mean Absolute Percentage Error (MAPE):** {mape:.4f}")
    st.write(f"ğŸ”¹ **RÂ² Score (R2):** {r2:.4f}")

    df_metrics = pd.DataFrame({
        "Metric": ["Test Loss (MSE)", "Mean Absolute Error (MAE)", "Mean Squared Error (MSE)",
                   "Root Mean Squared Error (RMSE)", "Mean Absolute Percentage Error (MAPE)", "RÂ² Score (R2)"],
        "Value": [test_loss, mae, mse, rmse, mape, r2]
    })
    st.dataframe(df_metrics)
else:
    st.error("à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸—à¸³à¸™à¸²à¸¢à¸œà¸¥à¹„à¸”à¹‰à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸ˆà¸²à¸à¹„à¸¡à¹ˆà¸à¸š scaler")