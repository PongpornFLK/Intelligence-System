import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
import numpy as np
import pickle
import os

def load_neural_model():
    # ðŸš€ 1. à¹‚à¸«à¸¥à¸” Dataset
    df = pd.read_csv(r'Data_set/education_career_bad_model.csv')
    
    # âœ… 2. à¸¥à¸šà¸„à¹ˆà¸²à¸œà¸´à¸”à¸›à¸à¸•à¸´ (Outliers) à¹ƒà¸™ Starting_Salary
    df = df[(df["Starting_Salary"] > 5000) & (df["Starting_Salary"] < 200000)]
    print(f"âœ… à¸‚à¸™à¸²à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸¥à¸±à¸‡à¸¥à¸š Outliers: {df.shape}")
    
    # âœ… 3. à¹€à¸•à¸´à¸¡à¸„à¹ˆà¸²à¸«à¸²à¸¢à¹„à¸›
    df.fillna(df.median(numeric_only=True), inplace=True)
    df.fillna(df.mode().iloc[0], inplace=True)  # à¹ƒà¸Šà¹‰ mode à¸ªà¸³à¸«à¸£à¸±à¸š Categorical
    
    # âœ… 4. à¹à¸›à¸¥à¸‡ Categorical Data à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸¥à¸‚à¸”à¹‰à¸§à¸¢ One-Hot Encoding
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
    
    # âœ… 5. à¹à¸¢à¸ Features (X) à¹à¸¥à¸° Target (y)
    X = df.drop(columns=["Starting_Salary"])
    y = np.log1p(df["Starting_Salary"])  # âœ… à¹ƒà¸Šà¹‰ Log Transform à¸¥à¸”à¸„à¸§à¸²à¸¡à¹€à¸šà¹‰à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    
    # âœ… 6. à¸—à¸³ Feature Scaling à¹à¸¥à¸°à¸šà¸±à¸™à¸—à¸¶à¸ Scaler
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Ensure the directory exists
    os.makedirs("TrainModel", exist_ok=True)
    
    with open("TrainModel/scaler.pkl", "wb") as f:
        pickle.dump(scaler, f)
    
    # âœ… 7. à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Train/Test
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    
    # ðŸš€ 8. à¸ªà¸£à¹‰à¸²à¸‡ MLP Model à¸—à¸µà¹ˆà¸›à¸£à¸±à¸šà¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡
    model = keras.Sequential([
        keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # à¹€à¸žà¸´à¹ˆà¸¡à¸ˆà¸³à¸™à¸§à¸™ Neurons
        keras.layers.Dropout(0.2),  # à¹ƒà¸Šà¹‰ Dropout à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ Overfitting
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(32, activation='relu'),
        keras.layers.Dense(1)  # Output Layer à¸ªà¸³à¸«à¸£à¸±à¸š Regression
    ])
    
    # ðŸš€ 9. à¸„à¸­à¸¡à¹„à¸žà¸¥à¹Œà¹‚à¸¡à¹€à¸”à¸¥
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                  loss='mse',
                  metrics=['mae'])
    
    # ðŸš€ 10. Train Model à¸žà¸£à¹‰à¸­à¸¡ Early Stopping
    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    history = model.fit(X_train, y_train,
                        epochs=100, batch_size=32,
                        validation_data=(X_test, y_test),
                        callbacks=[early_stopping], verbose=1)
    
    # âœ… 11. à¸šà¸±à¸™à¸—à¸¶à¸à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸°à¸„à¹ˆà¸²à¸›à¸£à¸°à¸§à¸±à¸•à¸´à¸à¸²à¸£ Train
    model.save("load_model.h5")
    with open("load_history.pkl", "wb") as f:
        pickle.dump(history.history, f)