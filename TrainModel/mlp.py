import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
import pickle
import os

# ðŸ“Œ à¸à¸³à¸«à¸™à¸” Paths à¸‚à¸­à¸‡à¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸šà¸±à¸™à¸—à¸¶à¸
MODEL_DIR = "TrainModel"
SCALER_PATH = os.path.join(MODEL_DIR, "scaler.pkl")
COLUMNS_PATH = os.path.join(MODEL_DIR, "train_columns.pkl")
MODEL_PATH = os.path.join(MODEL_DIR, "load_model.h5")
HISTORY_PATH = os.path.join(MODEL_DIR, "load_history.pkl")

# âœ… à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹à¸¥à¸°à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸–à¹‰à¸²à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ
os.makedirs(MODEL_DIR, exist_ok=True)

def load_neural_model():

    df = pd.read_csv(r'Data_set/education_career__model.csv' , nrows=1000)
    
    df = df.dropna()  # à¸¥à¸šà¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µà¸„à¹ˆà¸² NaN à¸—à¸´à¹‰à¸‡    

    df = df[(df["Starting_Salary"] > 5000) & (df["Starting_Salary"] < 200000)]

    for col in df.select_dtypes(include=["float64", "int64"]).columns:
        df[col].fillna(df[col].mean(), inplace=True)
    
    for col in df.select_dtypes(include=["object"]).columns:
        df[col].fillna(df[col].mode()[0], inplace=True)

    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
    df.dropna(inplace=True)

    feature_columns = df.drop(columns=["Starting_Salary"]).columns.tolist()
    with open(COLUMNS_PATH, "wb") as f:
        pickle.dump(feature_columns, f)

    X = df[feature_columns]
    y = np.log1p(df["Starting_Salary"])  # à¹ƒà¸Šà¹‰ Log Transform à¸¥à¸”à¸„à¸§à¸²à¸¡à¹€à¸šà¹‰à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    
    # à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸² X à¹à¸¥à¸° y à¸¡à¸µà¸„à¹ˆà¸² NaN à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
    if X.isnull().any().any() or y.isnull().any():
        print("âŒ à¸žà¸š NaN à¹ƒà¸™ X à¸«à¸£à¸·à¸­ y")
        return
    # à¸¥à¸šà¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µ NaN à¹ƒà¸™à¸—à¸±à¹‰à¸‡ X à¹à¸¥à¸° y à¸žà¸£à¹‰à¸­à¸¡à¸à¸±à¸™
    df_clean = pd.concat([X, y], axis=1).dropna()  # à¸£à¸§à¸¡ X à¹à¸¥à¸° y à¹à¸¥à¹‰à¸§à¸¥à¸šà¹à¸–à¸§à¸—à¸µà¹ˆà¸¡à¸µ NaN
    X_clean = df_clean[feature_columns]  # X à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸¥à¸š NaN
    y_clean = df_clean["Starting_Salary"]  # y à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸à¸¥à¸š NaN
    
    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¸™à¸²à¸”à¸‚à¸­à¸‡ X à¹à¸¥à¸° y
    if X_clean.shape[0] != y_clean.shape[0]:
        print(f"âŒ à¸‚à¸™à¸²à¸”à¸‚à¸­à¸‡ X à¹à¸¥à¸° y à¹„à¸¡à¹ˆà¹€à¸—à¹ˆà¸²à¸à¸±à¸™! X: {X_clean.shape[0]}, y: {y_clean.shape[0]}")
        return
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_clean)  # à¹ƒà¸Šà¹‰ X_clean à¸—à¸µà¹ˆà¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”à¹à¸¥à¹‰à¸§

    with open(SCALER_PATH, "wb") as f:
        pickle.dump(scaler, f)
        
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_clean, test_size=0.2, random_state=42)

    model = keras.Sequential([
        keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.3),

        keras.layers.Dense(128, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.3),

        keras.layers.Dense(64, activation='relu'),
        keras.layers.BatchNormalization(),
        keras.layers.Dropout(0.2),

        keras.layers.Dense(32, activation='relu'),
        keras.layers.BatchNormalization(),

        keras.layers.Dense(1)  # Output Layer à¸ªà¸³à¸«à¸£à¸±à¸š Regression
    ])

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                  loss='mse',
                  metrics=['mae'])

    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)

    history = model.fit(X_train, y_train,
                        epochs=150, batch_size=32,
                        validation_data=(X_test, y_test),
                        callbacks=[early_stopping, reduce_lr],
                        verbose=1)

    model.save(MODEL_PATH)
    with open(HISTORY_PATH, "wb") as f:
        pickle.dump(history.history, f)

